{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pkl file dumped by `gen_repr.py`\n",
    "rpath = '/data/ziyu/ood-cifar-inl-withclip.pkl'\n",
    "reprs = pickle.load(open(rpath, 'rb'))\n",
    "list(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(inl_train, ood, batch_dims=1):\n",
    "    inl_train = inl_train.reshape([np.prod(inl_train.shape[:batch_dims]), -1])\n",
    "    ood = ood.reshape([np.prod(ood.shape[:batch_dims]), -1])\n",
    "    _mean = inl_train.mean(axis=0)[None]\n",
    "    _sd = inl_train.std(axis=0)[None]\n",
    "    return (ood - _mean) / _sd\n",
    "\n",
    "def autocorr5(x, lags):\n",
    "    '''\n",
    "    adapted from https://stackoverflow.com/a/51168178/7509266\n",
    "    Fixed the incorrect denominator: np.correlate(x,x,'full')[d-1+k] returns\n",
    "        sum_{i=k}^{d-1} x[i]x[i-k]\n",
    "    so we should divide it by (d-k) instead of d\n",
    "    '''\n",
    "    mean=x.mean()\n",
    "    var=np.var(x)\n",
    "    xp=x-mean\n",
    "    ruler = len(x) - np.arange(len(x))\n",
    "    corr=np.correlate(xp,xp,'full')[len(x)-1:]/var/ruler\n",
    "    return corr[:(lags)]\n",
    "\n",
    "def get_autocorr(fea, B=None, L=200, test_typ='bp', skip_corr=1):\n",
    "    idcs = np.arange(fea.shape[0])\n",
    "    if B is not None:\n",
    "        np.random.shuffle(idcs)\n",
    "    else:\n",
    "        B = idcs.shape[0]\n",
    "    corrs = []\n",
    "    N = fea[0].shape[0]\n",
    "    for j in tqdm.trange(B):\n",
    "        ac_j = autocorr5(fea[idcs[j]], L)\n",
    "        corrs.append(ac_j)\n",
    "    corrs = np.array(corrs)[:,::skip_corr]\n",
    "    if test_typ == 'ljb':\n",
    "        ruler = (N - np.arange(1, L+1))[None, ::skip_corr].astype('f')\n",
    "        stats = N * (N+2) * (corrs[:,1:]**2 / ruler[:,1:]).mean(axis=-1)  # normalized; *L would follow ChiSq[L]\n",
    "    else:\n",
    "        stats = N * (corrs[:,1:]**2).astype('f').mean(axis=-1)\n",
    "    return stats, corrs\n",
    "\n",
    "def plot_autocorrelations(corrs):\n",
    "    plt.plot(np.arange(corrs.shape[1]), np.mean(corrs, axis=0))\n",
    "    plt.fill_between(np.arange(corrs.shape[1]),\n",
    "                     np.mean(corrs, axis=0)-np.std(corrs, axis=0), np.mean(corrs, axis=0)+np.std(corrs, axis=0), alpha=0.1)\n",
    "\n",
    "\n",
    "def autocorr_plot(fea, B=None, L=200, test_typ='bp', skip_corr=1):\n",
    "    stats, corrs = get_autocorr(fea, B, L, test_typ, skip_corr)\n",
    "    plot_autocorrelations(corrs)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LW = reprs['inl_train'][2].shape[1]; LW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def get_roc(fea_inl, fea_ood):\n",
    "    assert len(fea_inl.shape) == 1\n",
    "    y = np.concatenate([np.ones_like(fea_inl), np.zeros_like(fea_ood)], axis=0)\n",
    "    scores = np.concatenate([(fea_inl), (fea_ood)], axis=0)\n",
    "    fpr, tpr, thrs = roc_curve(y, scores)\n",
    "    auc = roc_auc_score(y, scores)\n",
    "    return fpr, tpr, auc\n",
    "\n",
    "def plot_roc(fea_inl, fea_ood):\n",
    "    fpr, tpr, auc = get_roc(fea_inl, fea_ood)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('false positive rate')\n",
    "    plt.ylabel('true positive rate')\n",
    "    plt.title('ROC curve, auc = {:.2f}'.format(auc))\n",
    "\n",
    "def proc_scores(inl, oul):\n",
    "    inl_mean = np.median(inl)\n",
    "    return -np.abs(inl-inl_mean), -np.abs(oul-inl_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_ms(TrLP, TLP, OOP, Bsz, Nsamples=10000, singleside=False):\n",
    "    TrLP_normalized = TrLP.reshape((-1,LW*LW)).mean(axis=-1)\n",
    "    TLP_normalized = TLP.reshape((-1,LW*LW)).mean(axis=-1)\n",
    "    OLP_normalized = OOP.reshape((-1,LW*LW)).mean(axis=-1)\n",
    "    \n",
    "    def proc_logp(logps, bsz):\n",
    "        N = logps.shape[0]\n",
    "        ss = []\n",
    "        for _ in range((Nsamples+N-1) * bsz // N):\n",
    "            idcs = np.arange(N)\n",
    "            np.random.shuffle(idcs)\n",
    "            logps = logps[idcs]\n",
    "            for k in range(0, N, bsz):\n",
    "                ss.append(logps[k: k+bsz].mean())\n",
    "        return np.array(ss)\n",
    "    \n",
    "    aucs = []\n",
    "    for bsz in Bsz:\n",
    "        inl_s = proc_logp(TLP_normalized, bsz)\n",
    "        oul_s = proc_logp(OLP_normalized, bsz)\n",
    "        if not singleside:\n",
    "            _, _, auc = get_roc(*proc_scores(inl_s, oul_s))\n",
    "        else:\n",
    "            _, _, auc = get_roc(inl_s, oul_s)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_test(\n",
    "    inl_train_fea, inl_test_fea, oul_fea, test_typ, Bsz=1, Nsamples=None,\n",
    "    L=100, SK=1):\n",
    "    \"\"\"\n",
    "    :param Bsz: batch size in a multi-sample test (as in the multi-sample typicality test, arXiv:1906.02994).\n",
    "    :param SK: only include lags which are multiples of SK in the test\n",
    "    :param L: the maximum lag to use\n",
    "    \"\"\"\n",
    "    oul_fea = normalize_feature(inl_train_fea, oul_fea)\n",
    "    inl_fea = normalize_feature(inl_train_fea, inl_test_fea)\n",
    "    oul_stats, _ = get_autocorr(oul_fea, Nsamples, L, test_typ, SK)\n",
    "    inl_stats, _ = get_autocorr(inl_fea, Nsamples, L, test_typ, SK)\n",
    "    return get_roc(-inl_stats, -oul_stats)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Proposed WN Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BSZ = [1]\n",
    "\n",
    "for k in list(reprs):\n",
    "    if k.startswith('od'):\n",
    "        ktrain = 'inl_train'\n",
    "        ktest = 'inl_test'\n",
    "    elif k == 'inl_test':\n",
    "        ktrain = ktest = 'inl_train'\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "    auc = time_series_test(\n",
    "        reprs[ktrain][1][...], reprs[ktest][1][...], reprs[k][1][...], 'bp',\n",
    "        Bsz=BSZ, L=400*3, SK=LW*3, normalize=True)\n",
    "    print(k, auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "## The Likelihood Tests\n",
    "\n",
    "You can modify `BSZ` below to run the multi-sample typicality test.\n",
    "\n",
    "### 2-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSZ = [1]\n",
    "\n",
    "for k in list(reprs):\n",
    "    if k.startswith('od'):\n",
    "        ktrain = 'inl_train'\n",
    "        ktest = 'inl_test'\n",
    "    elif k == 'inl_test':\n",
    "        ktrain = ktest = 'inl_train'\n",
    "    else:\n",
    "        continue\n",
    "    aucs = logp_ms(reprs[ktrain][0], reprs[ktest][0], reprs[k][0], BSZ, singleside=False)\n",
    "    print(k, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(reprs):\n",
    "    if k.startswith('od'):\n",
    "        ktrain = 'inl_train'\n",
    "        ktest = 'inl_test'\n",
    "    elif k == 'inl_test':\n",
    "        ktrain = ktest = 'inl_train'\n",
    "    else:\n",
    "        continue\n",
    "    aucs = logp_ms(reprs[ktrain][0], reprs[ktest][0], reprs[k][0], BSZ, singleside=True)\n",
    "    print(k, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Compression Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `discretized_mix_logistic_loss` returns log likelihood in nats. The log likelihood dumped by `gen_reprs.py` is a NHW tensor with each \n",
    "# dimension corresponding to the log joint pdf of the three subpixels.\n",
    "# Thus in the following, we divide the input logp by 3, negate it to get nats-per-dimension, and subtract `BPD_PNG * ln(2) = NatsPD_PNG`. \n",
    "# The result is the negated LR test statistics, so the higher it is, the more likely the input is considered as outlier.\n",
    "\n",
    "def compression_stats(logp, orig_ims):\n",
    "    stats = []\n",
    "    LW = orig_ims.shape[1]\n",
    "    for k in tqdm.trange(orig_ims.shape[0]):\n",
    "        rval, buf = cv2.imencode('.png', orig_ims[k], [int(cv2.IMWRITE_PNG_COMPRESSION), 9])\n",
    "        assert rval, \"PNG compression failed\"\n",
    "        bpd_png = 8 * buf.shape[0] / (LW*LW*3)\n",
    "        stats.append(-logp[k].mean() / 3 - bpd_png * np.log(2))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inl_train_compression_stats = compression_stats(reprs['inl_train'][0], reprs['inl_train'][2])\n",
    "inl_test_compression_stats = compression_stats(reprs['inl_test'][0], reprs['inl_test'][2])\n",
    "print('inlier train vs test',\n",
    "      get_roc(-np.array(inl_train_compression_stats), -np.array(inl_test_compression_stats))[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(reprs):\n",
    "    if k.startswith('od'):\n",
    "        k_compression_stats = compression_stats(reprs[k][0], reprs[k][2])\n",
    "        auc = get_roc(-np.array(inl_test_compression_stats), -np.array(k_compression_stats))[2]\n",
    "        print(k, auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and Diagnostics\n",
    "\n",
    "\n",
    "## Distribution of ACFs (Fig. 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rpath.find('cifar-') != -1:\n",
    "    oul_fea = normalize_feature(reprs['inl_train'][1][...], reprs['od_svhn'][1][...])\n",
    "    inl_fea = normalize_feature(reprs['inl_train'][1][...], reprs['inl_test'][1][...])\n",
    "    plt.figure(figsize=(7,2.5), facecolor='w')\n",
    "    plt.subplot(121)\n",
    "    plt.title('outlier ACF')\n",
    "    oul_stats = autocorr_plot(oul_fea, L=500, B=5000, test_typ='bp', skip_corr=1, normalize=True)\n",
    "    X = np.arange(500)\n",
    "    plt.plot(X, 1/np.sqrt(1024*3)*np.ones((500,)), linestyle='--', color='gray')\n",
    "    plt.plot(X, -1/np.sqrt(1024*3)*np.ones((500,)), linestyle='--', color='gray')\n",
    "    plt.ylim(-0.05, 0.25)\n",
    "    plt.subplot(122)\n",
    "    plt.title('inlier ACF')\n",
    "    inl_stats = autocorr_plot(inl_fea, L=500, B=5000, test_typ='bp', skip_corr=1, normalize=True)\n",
    "    plt.ylim(-0.05, 0.25)\n",
    "    plt.plot(X, 1/np.sqrt(1024*3)*np.ones((500,)), linestyle='--', color='gray')\n",
    "    plt.plot(X, -1/np.sqrt(1024*3)*np.ones((500,)), linestyle='--', color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity to L (Fig. 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rpath.find('cifar-') != -1:\n",
    "    \n",
    "    # NOTE: we are using a small subset of data to speed up the process. There will be an error of ~0.03.\n",
    "    # Fig.5 was generated with Nsamples=None\n",
    "    Nsamples = 1000\n",
    "\n",
    "    ac_results = {}\n",
    "\n",
    "    for k in list(reprs):\n",
    "        if k.startswith('od'):\n",
    "            ktrain = 'inl_train'\n",
    "            ktest = 'inl_test'\n",
    "        elif k == 'inl_test':\n",
    "            ktrain = ktest = 'inl_train'\n",
    "        else:\n",
    "            continue\n",
    "        cr = []\n",
    "        for L in [200, 400, 700]:  # max 1024=32x32\n",
    "            auc = time_series_test(\n",
    "                reprs[ktrain][1][...], reprs[ktest][1][...], reprs[k][1][...], 'bp',\n",
    "                Nsamples=Nsamples, Bsz=BSZ, L=L*3, SK=LW*3, normalize=True)\n",
    "            cr.append(auc)\n",
    "        ac_results[k] = cr\n",
    "        \n",
    "    plt.figure(figsize=(10,2), facecolor='w')\n",
    "    j = 0\n",
    "    for k in ac_results:\n",
    "        if k != 'od_celeba32' and k != 'od_imagenet' and k != 'od_svhn':\n",
    "            continue\n",
    "        j += 1\n",
    "        plt.subplot(1, 3, j)\n",
    "        plt.plot([200*3, 400*3, 700*3], ac_results[k], '-+')\n",
    "        plt.xlabel('L')\n",
    "        if j == 1:\n",
    "            plt.ylabel('AUROC')\n",
    "        dispname = {\n",
    "            'od_celeba32': 'CelebA',\n",
    "            'od_imagenet': 'TinyImageNet',\n",
    "            'od_svhn': 'SVHN'\n",
    "        }[k]\n",
    "        plt.title(dispname)\n",
    "        #plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phist(inp, lbl): _ = plt.hist(inp, bins=100, alpha=0.4, density=True, label=lbl)\n",
    "\n",
    "if 'od_synth-bus_sep' in reprs:\n",
    "    plt.figure(figsize=(8,2))\n",
    "    plt.subplot(131)\n",
    "    phist(np.log(2) * -reprs['inl_test'][0].mean(axis=(1,2))/3, 'Inlier')\n",
    "    phist(np.log(2) * -reprs['od_synth-bus_sep'][0].mean(axis=(1,2))/3, 'Outlier')\n",
    "    plt.title('Model BPD')\n",
    "    plt.legend()\n",
    "    plt.subplot(132)\n",
    "    phist(np.log(2) * (-np.array(inl_test_compression_stats)-reprs['inl_test'][0].mean(axis=(1,2))/3), 'Inlier')\n",
    "    phist(np.log(2) * (-np.array(k_compression_stats)-reprs['od_synth-bus_sep'][0].mean(axis=(1,2))/3), 'Outlier')\n",
    "    #plt.legend()\n",
    "    plt.title('Generic BPD')\n",
    "    plt.subplot(133)\n",
    "    phist(np.log(2) * (-np.array(inl_test_compression_stats)), 'Inlier')\n",
    "    phist(np.log(2) * (-np.array(k_compression_stats)), 'Outlier')\n",
    "    #plt.legend()\n",
    "    plt.title('Test Stats')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixelcnn",
   "language": "python",
   "name": "pixelcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
